seed_everything: null
trainer:
  gpus: 1
  logger: true
  checkpoint_callback: null
  enable_checkpointing: true
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: code_gnn.periodic_checkpoint.PeriodicModelCheckpoint
      init_args:
        every: 25
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        filename: "performance-{epoch:02d}-{step:02d}-{val_loss:02f}"
        monitor: val_loss
        mode: min
        save_last: true
        save_top_k: 5
  default_root_dir: null
  enable_progress_bar: true
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 1
  max_epochs: 500
  weights_summary: top
  weights_save_path: null
  resume_from_checkpoint: null
  detect_anomaly: true
  gradient_clip_val: null
  accumulate_grad_batches: null
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-3
    weight_decay: 1e-2
data:
  feat: _ABS_DATAFLOW_datatype_all
  gtype: "cfg"
model:
  num_layers: 5
  num_mlp_layers: 2
  hidden_dim: 32
  final_dropout: 0.5
  learn_eps: false
  graph_pooling_type: sum
  neighbor_pooling_type: sum
  separate_embedding_layer: false
ckpt_path: null
